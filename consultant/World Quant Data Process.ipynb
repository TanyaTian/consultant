{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T03:46:26.688300Z",
     "start_time": "2025-04-02T03:46:26.199886Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from machine_lib_output import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc8c63d3f3c14cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T03:46:33.507541Z",
     "start_time": "2025-04-02T03:46:32.223123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"user\":{\"id\":\"YT33830\"},\"token\":{\"expiry\":14400.0},\"permissions\":[\"BEFORE_AND_AFTER_PERFORMANCE_V2\",\"BRAIN_LABS\",\"BRAIN_LABS_JUPYTER_LAB\",\"CONSULTANT\",\"MULTI_SIMULATION\",\"PROD_ALPHAS\",\"REFERRAL\",\"SUPER_ALPHA\",\"VISUALIZATION\",\"WORKDAY\"]}'\n"
     ]
    }
   ],
   "source": [
    "s = login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bddd7ae23161c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T04:49:43.335032Z",
     "start_time": "2025-04-01T04:49:34.651767Z"
    }
   },
   "outputs": [],
   "source": [
    "categories = ['analyst', 'insiders', 'fundamental', 'macro', 'model', 'news', 'option', 'other', 'pv', 'risk', 'sentiment', 'socialmedia', 'earnings']\n",
    "output_dir = \"data\"\n",
    "output_file = os.path.join(output_dir, 'USA/categories_delay1_illiquid.csv')\n",
    "# 自定义参数，覆盖部分默认值\n",
    "\n",
    "default_params = {\n",
    "            'delay': 1,\n",
    "            'instrumentType': 'EQUITY',\n",
    "            'limit': 30,\n",
    "            'offset': 0,\n",
    "            'region': 'USA',\n",
    "            'universe': 'ILLIQUID_MINVOL1M'\n",
    "        }\n",
    "for category in categories:\n",
    "        params = default_params.copy()\n",
    "        params['category'] = category\n",
    "        results_df = fetch_data(s, params)\n",
    "        if not results_df.empty:\n",
    "            write_to_csv(results_df, output_file)\n",
    "            print(f\"类别 '{category}' 的数据已追加到 {output_file}\")\n",
    "        else:\n",
    "            print(f\"类别 '{category}' 未返回数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ac9c80bfd3253",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T06:39:00.330756Z",
     "start_time": "2025-04-02T06:38:55.541700Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_ids = ['analyst14', 'fundamental6', 'analyst48', 'analyst14', 'news18', 'news87', 'model26', 'model29','model30', 'model51']\n",
    "output_dir = \"data\"\n",
    "output_data_fields = os.path.join(output_dir, 'datafields5.csv')\n",
    "for dataset_id in dataset_ids:\n",
    "    df = get_datafields(s, dataset_id = dataset_id, region='USA', universe='TOP3000', delay=1)\n",
    "    df = df[df['type'] == 'MATRIX']\n",
    "    if not df.empty:\n",
    "        write_to_csv(df, output_data_fields)\n",
    "        print(f\"类别 '{dataset_id}' 的数据已追加到 {output_data_fields}\")\n",
    "    else:\n",
    "        print(f\"类别 '{dataset_id}' 未返回数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0263b53a6f13453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取的 dataset_ids: ['analyst11', 'analyst14', 'analyst15', 'analyst47', 'analyst48', 'analyst69', 'analyst82', 'model211', 'fundamental1', 'fundamental17', 'fundamental23', 'fundamental28', 'fundamental31', 'fundamental44', 'fundamental45', 'fundamental6', 'fundamental72', 'macro27', 'model106', 'model109', 'model110', 'model138', 'model239', 'model242', 'model26', 'model262', 'model264', 'model30', 'model38', 'model53', 'model77', 'other432', 'other460', 'other54', 'shortinterest7', 'news20', 'news23', 'news52', 'news76', 'other455', 'pv1', 'pv13', 'pv17', 'pv29', 'pv37', 'pv53', 'pv96', 'univ1', 'risk60', 'risk70', 'earnings3', 'earnings5']\n",
      "类别 'analyst11' 未返回数据\n",
      "类别 'analyst14' 未返回数据\n",
      "类别 'analyst15' 未返回数据\n",
      "类别 'analyst47' 未返回数据\n",
      "类别 'analyst48' 未返回数据\n",
      "类别 'analyst69' 未返回数据\n",
      "类别 'analyst82' 未返回数据\n",
      "类别 'model211' 未返回数据\n",
      "类别 'fundamental1' 未返回数据\n",
      "类别 'fundamental17' 未返回数据\n",
      "类别 'fundamental23' 未返回数据\n",
      "类别 'fundamental28' 未返回数据\n",
      "类别 'fundamental31' 未返回数据\n",
      "类别 'fundamental44' 未返回数据\n",
      "类别 'fundamental45' 未返回数据\n",
      "类别 'fundamental6' 未返回数据\n",
      "类别 'fundamental72' 未返回数据\n",
      "类别 'macro27' 未返回数据\n",
      "类别 'model106' 未返回数据\n",
      "类别 'model109' 未返回数据\n",
      "类别 'model110' 未返回数据\n",
      "类别 'model138' 未返回数据\n",
      "类别 'model239' 未返回数据\n",
      "类别 'model242' 未返回数据\n",
      "类别 'model26' 未返回数据\n",
      "类别 'model262' 未返回数据\n",
      "类别 'model264' 未返回数据\n",
      "类别 'model30' 未返回数据\n",
      "类别 'model38' 未返回数据\n",
      "类别 'model53' 未返回数据\n",
      "类别 'model77' 未返回数据\n",
      "类别 'other432' 未返回数据\n",
      "类别 'other460' 未返回数据\n",
      "类别 'other54' 未返回数据\n",
      "类别 'shortinterest7' 未返回数据\n",
      "类别 'news20' 未返回数据\n",
      "类别 'news23' 未返回数据\n",
      "类别 'news52' 未返回数据\n",
      "类别 'news76' 未返回数据\n",
      "类别 'other455' 的数据已写入 data/GLB/other455_group.csv\n",
      "类别 'pv1' 的数据已写入 data/GLB/pv1_group.csv\n",
      "类别 'pv13' 的数据已写入 data/GLB/pv13_group.csv\n",
      "类别 'pv17' 的数据已写入 data/GLB/pv17_group.csv\n",
      "类别 'pv29' 的数据已写入 data/GLB/pv29_group.csv\n",
      "类别 'pv37' 未返回数据\n",
      "类别 'pv53' 未返回数据\n",
      "类别 'pv96' 未返回数据\n",
      "类别 'univ1' 未返回数据\n",
      "类别 'risk60' 未返回数据\n",
      "类别 'risk70' 未返回数据\n",
      "类别 'earnings3' 未返回数据\n",
      "类别 'earnings5' 未返回数据\n"
     ]
    }
   ],
   "source": [
    "# 数据集 ID 列表\n",
    "\"\"\"\n",
    "dataset_ids = ['analyst14', 'analyst15', 'analyst39', 'analyst4', \n",
    "               'analyst40', 'analyst69', 'analyst7', 'analyst81', \n",
    "               'analyst9', 'model52', 'model26', 'model30', 'fundamental17',\n",
    "               'fundamental28', 'fundamental6', 'news18', 'pv1']\n",
    "\"\"\"\n",
    "output_dir = \"data/GLB\"\n",
    "\n",
    "# 确保输出目录存在\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"创建输出目录: {output_dir}\")\n",
    "    \n",
    "csv_file_path = os.path.join(output_dir, \"categories_delay1.csv\")  # 替换为你的 CSV 文件路径\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 提取 id 列，生成 dataset_ids 列表\n",
    "dataset_ids = df['id'].tolist()\n",
    "\n",
    "# 打印 dataset_ids，确认结果\n",
    "print(\"提取的 dataset_ids:\", dataset_ids)\n",
    "\n",
    "# 假设 s 是一个已定义的会话对象（例如 requests.Session）\n",
    "# s = requests.Session()\n",
    "\n",
    "# 遍历 dataset_ids，为每个 dataset_id 生成一个单独的 CSV 文件\n",
    "for dataset_id in dataset_ids:\n",
    "    # 构造输出文件路径，例如 data/analyst14.csv\n",
    "    output_file = os.path.join(output_dir, f\"{dataset_id}_group.csv\")\n",
    "    \n",
    "    # 获取数据\n",
    "    df = get_datafields(s, dataset_id=dataset_id, region='GLB', universe='TOP3000', delay=1)\n",
    "    if not df.empty:\n",
    "        # 过滤 type 为 'GROUP' 的数据\n",
    "        df = df[df['type'] == 'GROUP']\n",
    "        \n",
    "        # 检查 DataFrame 是否为空\n",
    "        if not df.empty:\n",
    "            # 写入 CSV 文件\n",
    "            write_to_csv(df, output_file)\n",
    "            print(f\"类别 '{dataset_id}' 的数据已写入 {output_file}\")\n",
    "        else:\n",
    "            print(f\"类别 '{dataset_id}' 未返回数据\")\n",
    "    else:\n",
    "        print(f\"类别 '{dataset_id}' 未返回数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cdd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 输入参数\n",
    "dataset_ids = ['fundamental6']  # 需要合并的 dataset_ids\n",
    "input_dir = \"data/USA\"  # CSV 文件所在的目录\n",
    "output_dir = \"output\"\n",
    "output_file = os.path.join(output_dir, \"fundamental6.txt\")  # 合并后的 TXT 文件路径\n",
    "\n",
    "# 存储所有数据的 DataFrame 列表\n",
    "dfs = []\n",
    "\n",
    "# 遍历 dataset_ids，读取对应的 CSV 文件\n",
    "for dataset_id in dataset_ids:\n",
    "    csv_file = os.path.join(input_dir, f\"{dataset_id}.csv\")\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"文件 {csv_file} 不存在，跳过\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # 读取 CSV 文件\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "        print(f\"已读取文件: {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件 {csv_file} 时发生错误: {e}\")\n",
    "        continue\n",
    "\n",
    "# 检查是否有文件被成功读取\n",
    "if not dfs:\n",
    "    print(\"没有成功读取任何文件，无法合并\")\n",
    "else:\n",
    "    # 合并所有 DataFrame\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # 将合并后的数据写入 TXT 文件（以 CSV 格式写入）\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"已将 {len(dfs)} 个文件合并到 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef32ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_csv('data/EUR/fundamental23_matrix.csv', 5, 'data/EUR/part')\n",
    "#extract_id_description('/Users/sujianan/py/consultant/consultant/data/USA/analyst15_matrix.csv', '/Users/sujianan/py/consultant/consultant/data/USA/part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e3ec55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/alpha_list_pending_simulated_2_filter.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_csv_by_keywords('output/alpha_list_pending_simulated_2.csv', ['mdl230_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc5be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
